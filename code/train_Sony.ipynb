{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7338,"status":"ok","timestamp":1680554631391,"user":{"displayName":"Remco H.","userId":"12103293379815967347"},"user_tz":-120},"id":"86cnW51WDf5d","outputId":"efbd055c-e938-44a9-9b37-43b471710221"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (1.22.4)\n","Collecting rawpy\n","  Downloading rawpy-0.18.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tensorflow in /usr/local/lib/python3.9/dist-packages (2.12.0)\n","Requirement already satisfied: tf_slim in /usr/local/lib/python3.9/dist-packages (1.1.0)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.32.0)\n","Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.4.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.8.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.4.7)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.53.0)\n","Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (23.3.3)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (16.0.0)\n","Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.12.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (4.5.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.2.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.16.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.3.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow) (67.6.1)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow) (23.0)\n","Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.12.0)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.20.3)\n","Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.12.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\n","Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.15->tensorflow) (1.10.1)\n","Requirement already satisfied: ml-dtypes>=0.0.3 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.15->tensorflow) (0.0.4)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.4.6)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.8.1)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.2.3)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.27.1)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.3)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.2.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow) (6.1.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.0.12)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2022.12.7)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow) (3.15.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n","Installing collected packages: rawpy\n","Successfully installed rawpy-0.18.0\n"]}],"source":["!pip install numpy rawpy tensorflow tf_slim"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":478},"id":"JEXgd64HB7Gx","executionInfo":{"status":"error","timestamp":1680554687396,"user_tz":-120,"elapsed":51602,"user":{"displayName":"Remco H.","userId":"12103293379815967347"}},"outputId":"0bdadbde-2f53-4ea4-e7cf-536f630e5278"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:From /usr/local/lib/python3.9/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","non-resource variables are not supported in the long term\n"]},{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n","  warnings.warn('`layer.apply` is deprecated and '\n"]},{"output_type":"stream","name":"stdout","text":["File started at 20:44:00, now at 20:44:42.\n","1 1 Loss=0.160 Time=14.665\n","File started at 20:44:00, now at 20:44:46.\n","1 2 Loss=0.228 Time=3.250\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-ce86c134ca0d>\u001b[0m in \u001b[0;36m<cell line: 338>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0mtrain_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0min_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'%05d_00*.ARW'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtrain_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m         \u001b[0min_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0min_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_files\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m         \u001b[0min_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.randint\u001b[0;34m()\u001b[0m\n","\u001b[0;32m_bounded_integers.pyx\u001b[0m in \u001b[0;36mnumpy.random._bounded_integers._rand_int64\u001b[0;34m()\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: high <= 0"]}],"source":["# uniform content loss + adaptive threshold + per_class_input + recursive G\n","# improvement upon cqf37\n","from __future__ import division\n","import os, time, scipy.io\n","#import tensorflow as tf\n","#import tensorflow.contrib.slim as slim\n","import numpy as np\n","import rawpy\n","import glob\n","import random\n","\n","#Hi i added this\n","import tf_slim as slim\n","import tensorflow.compat.v1 as tf\n","tf.disable_v2_behavior()\n","from PIL import Image\n","\n","start_time = time.strftime(\"%H:%M:%S\", time.gmtime(time.time()))\n","\n","# Google colab finding files:\n","from google.colab import drive\n","drive.mount(\"/content/drive\")\n","\n","# Note that you need a link to the shared folder in your google drive!\n","project_dir = '/content/drive/MyDrive/Deep Learning Project - Learning to see in the dark/'\n","link_dir = project_dir + 'Images/OG dataset curated subset/Sony/'\n","run_name = 'pretrained_'\n","\n","input_dir = link_dir + 'short/'\n","gt_dir = link_dir + 'long/'\n","checkpoint_dir = project_dir + run_name + 'models/'\n","result_dir = project_dir + run_name + 'results/'\n","\n","# get train IDs\n","train_fns = glob.glob(gt_dir + '0*.ARW')\n","train_ids = [int(os.path.basename(train_fn)[0:5]) for train_fn in train_fns]\n","\n","ps = 512  # patch size for training\n","save_freq = 500\n","\n","DEBUG = 0\n","if DEBUG == 1:\n","    save_freq = 2\n","    train_ids = train_ids[0:5]\n","    \n","_errstr = \"Mode is unknown or incompatible with input array shape.\"\n","\n","# https://stackoverflow.com/questions/57545125/attributeerror-module-scipy-misc-has-no-attribute-toimage\n","def bytescale(data, cmin=None, cmax=None, high=255, low=0):\n","    \"\"\"\n","    Byte scales an array (image).\n","    Byte scaling means converting the input image to uint8 dtype and scaling\n","    the range to ``(low, high)`` (default 0-255).\n","    If the input image already has dtype uint8, no scaling is done.\n","    This function is only available if Python Imaging Library (PIL) is installed.\n","    Parameters\n","    ----------\n","    data : ndarray\n","        PIL image data array.\n","    cmin : scalar, optional\n","        Bias scaling of small values. Default is ``data.min()``.\n","    cmax : scalar, optional\n","        Bias scaling of large values. Default is ``data.max()``.\n","    high : scalar, optional\n","        Scale max value to `high`.  Default is 255.\n","    low : scalar, optional\n","        Scale min value to `low`.  Default is 0.\n","    Returns\n","    -------\n","    img_array : uint8 ndarray\n","        The byte-scaled array.\n","    Examples\n","    --------\n","    >>> from scipy.misc import bytescale\n","    >>> img = np.array([[ 91.06794177,   3.39058326,  84.4221549 ],\n","    ...                 [ 73.88003259,  80.91433048,   4.88878881],\n","    ...                 [ 51.53875334,  34.45808177,  27.5873488 ]])\n","    >>> bytescale(img)\n","    array([[255,   0, 236],\n","           [205, 225,   4],\n","           [140,  90,  70]], dtype=uint8)\n","    >>> bytescale(img, high=200, low=100)\n","    array([[200, 100, 192],\n","           [180, 188, 102],\n","           [155, 135, 128]], dtype=uint8)\n","    >>> bytescale(img, cmin=0, cmax=255)\n","    array([[91,  3, 84],\n","           [74, 81,  5],\n","           [52, 34, 28]], dtype=uint8)\n","    \"\"\"\n","    if data.dtype == np.uint8:\n","        return data\n","\n","    if high > 255:\n","        raise ValueError(\"`high` should be less than or equal to 255.\")\n","    if low < 0:\n","        raise ValueError(\"`low` should be greater than or equal to 0.\")\n","    if high < low:\n","        raise ValueError(\"`high` should be greater than or equal to `low`.\")\n","\n","    if cmin is None:\n","        cmin = data.min()\n","    if cmax is None:\n","        cmax = data.max()\n","\n","    cscale = cmax - cmin\n","    if cscale < 0:\n","        raise ValueError(\"`cmax` should be larger than `cmin`.\")\n","    elif cscale == 0:\n","        cscale = 1\n","\n","    scale = float(high - low) / cscale\n","    bytedata = (data - cmin) * scale + low\n","    return (bytedata.clip(low, high) + 0.5).astype(np.uint8)\n","\n","\n","def toimage(arr, high=255, low=0, cmin=None, cmax=None, pal=None,\n","            mode=None, channel_axis=None):\n","    \"\"\"Takes a numpy array and returns a PIL image.\n","    This function is only available if Python Imaging Library (PIL) is installed.\n","    The mode of the PIL image depends on the array shape and the `pal` and\n","    `mode` keywords.\n","    For 2-D arrays, if `pal` is a valid (N,3) byte-array giving the RGB values\n","    (from 0 to 255) then ``mode='P'``, otherwise ``mode='L'``, unless mode\n","    is given as 'F' or 'I' in which case a float and/or integer array is made.\n","    .. warning::\n","        This function uses `bytescale` under the hood to rescale images to use\n","        the full (0, 255) range if ``mode`` is one of ``None, 'L', 'P', 'l'``.\n","        It will also cast data for 2-D images to ``uint32`` for ``mode=None``\n","        (which is the default).\n","    Notes\n","    -----\n","    For 3-D arrays, the `channel_axis` argument tells which dimension of the\n","    array holds the channel data.\n","    For 3-D arrays if one of the dimensions is 3, the mode is 'RGB'\n","    by default or 'YCbCr' if selected.\n","    The numpy array must be either 2 dimensional or 3 dimensional.\n","    \"\"\"\n","    data = np.asarray(arr)\n","    if np.iscomplexobj(data):\n","        raise ValueError(\"Cannot convert a complex-valued array.\")\n","    shape = list(data.shape)\n","    valid = len(shape) == 2 or ((len(shape) == 3) and\n","                                ((3 in shape) or (4 in shape)))\n","    if not valid:\n","        raise ValueError(\"'arr' does not have a suitable array shape for \"\n","                         \"any mode.\")\n","    if len(shape) == 2:\n","        shape = (shape[1], shape[0])  # columns show up first\n","        if mode == 'F':\n","            data32 = data.astype(np.float32)\n","            image = Image.frombytes(mode, shape, data32.tostring())\n","            return image\n","        if mode in [None, 'L', 'P']:\n","            bytedata = bytescale(data, high=high, low=low,\n","                                 cmin=cmin, cmax=cmax)\n","            image = Image.frombytes('L', shape, bytedata.tostring())\n","            if pal is not None:\n","                image.putpalette(np.asarray(pal, dtype=np.uint8).tostring())\n","                # Becomes a mode='P' automagically.\n","            elif mode == 'P':  # default gray-scale\n","                pal = (np.arange(0, 256, 1, dtype=np.uint8)[:, np.newaxis] *\n","                       np.ones((3,), dtype=np.uint8)[np.newaxis, :])\n","                image.putpalette(np.asarray(pal, dtype=np.uint8).tostring())\n","            return image\n","        if mode == '1':  # high input gives threshold for 1\n","            bytedata = (data > high)\n","            image = Image.frombytes('1', shape, bytedata.tostring())\n","            return image\n","        if cmin is None:\n","            cmin = np.amin(np.ravel(data))\n","        if cmax is None:\n","            cmax = np.amax(np.ravel(data))\n","        data = (data*1.0 - cmin)*(high - low)/(cmax - cmin) + low\n","        if mode == 'I':\n","            data32 = data.astype(np.uint32)\n","            image = Image.frombytes(mode, shape, data32.tostring())\n","        else:\n","            raise ValueError(_errstr)\n","        return image\n","\n","    # if here then 3-d array with a 3 or a 4 in the shape length.\n","    # Check for 3 in datacube shape --- 'RGB' or 'YCbCr'\n","    if channel_axis is None:\n","        if (3 in shape):\n","            ca = np.flatnonzero(np.asarray(shape) == 3)[0]\n","        else:\n","            ca = np.flatnonzero(np.asarray(shape) == 4)\n","            if len(ca):\n","                ca = ca[0]\n","            else:\n","                raise ValueError(\"Could not find channel dimension.\")\n","    else:\n","        ca = channel_axis\n","\n","    numch = shape[ca]\n","    if numch not in [3, 4]:\n","        raise ValueError(\"Channel axis dimension is not valid.\")\n","\n","    bytedata = bytescale(data, high=high, low=low, cmin=cmin, cmax=cmax)\n","    if ca == 2:\n","        strdata = bytedata.tobytes()\n","        shape = (shape[1], shape[0])\n","    elif ca == 1:\n","        strdata = np.transpose(bytedata, (0, 2, 1)).tobytes()\n","        shape = (shape[2], shape[0])\n","    elif ca == 0:\n","        strdata = np.transpose(bytedata, (1, 2, 0)).tobytes()\n","        shape = (shape[2], shape[1])\n","    if mode is None:\n","        if numch == 3:\n","            mode = 'RGB'\n","        else:\n","            mode = 'RGBA'\n","\n","    if mode not in ['RGB', 'RGBA', 'YCbCr', 'CMYK']:\n","        raise ValueError(_errstr)\n","\n","    if mode in ['RGB', 'YCbCr']:\n","        if numch != 3:\n","            raise ValueError(\"Invalid array shape for mode.\")\n","    if mode in ['RGBA', 'CMYK']:\n","        if numch != 4:\n","            raise ValueError(\"Invalid array shape for mode.\")\n","\n","    # Here we know data and mode is correct\n","    image = Image.frombytes(mode, shape, strdata)\n","    return image\n","\n","def lrelu(x):\n","    return tf.maximum(x * 0.2, x)\n","\n","\n","def upsample_and_concat(x1, x2, output_channels, in_channels):\n","    pool_size = 2\n","    deconv_filter = tf.Variable(tf.truncated_normal([pool_size, pool_size, output_channels, in_channels], stddev=0.02))\n","    deconv = tf.nn.conv2d_transpose(x1, deconv_filter, tf.shape(x2), strides=[1, pool_size, pool_size, 1])\n","\n","    deconv_output = tf.concat([deconv, x2], 3)\n","    deconv_output.set_shape([None, None, None, output_channels * 2])\n","\n","    return deconv_output\n","\n","\n","def network(input):\n","    conv1 = slim.conv2d(input, 32, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv1_1')\n","    conv1 = slim.conv2d(conv1, 32, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv1_2')\n","    pool1 = slim.max_pool2d(conv1, [2, 2], padding='SAME')\n","\n","    conv2 = slim.conv2d(pool1, 64, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv2_1')\n","    conv2 = slim.conv2d(conv2, 64, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv2_2')\n","    pool2 = slim.max_pool2d(conv2, [2, 2], padding='SAME')\n","\n","    conv3 = slim.conv2d(pool2, 128, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv3_1')\n","    conv3 = slim.conv2d(conv3, 128, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv3_2')\n","    pool3 = slim.max_pool2d(conv3, [2, 2], padding='SAME')\n","\n","    conv4 = slim.conv2d(pool3, 256, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv4_1')\n","    conv4 = slim.conv2d(conv4, 256, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv4_2')\n","    pool4 = slim.max_pool2d(conv4, [2, 2], padding='SAME')\n","\n","    conv5 = slim.conv2d(pool4, 512, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv5_1')\n","    conv5 = slim.conv2d(conv5, 512, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv5_2')\n","\n","    up6 = upsample_and_concat(conv5, conv4, 256, 512)\n","    conv6 = slim.conv2d(up6, 256, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv6_1')\n","    conv6 = slim.conv2d(conv6, 256, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv6_2')\n","\n","    up7 = upsample_and_concat(conv6, conv3, 128, 256)\n","    conv7 = slim.conv2d(up7, 128, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv7_1')\n","    conv7 = slim.conv2d(conv7, 128, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv7_2')\n","\n","    up8 = upsample_and_concat(conv7, conv2, 64, 128)\n","    conv8 = slim.conv2d(up8, 64, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv8_1')\n","    conv8 = slim.conv2d(conv8, 64, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv8_2')\n","\n","    up9 = upsample_and_concat(conv8, conv1, 32, 64)\n","    conv9 = slim.conv2d(up9, 32, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv9_1')\n","    conv9 = slim.conv2d(conv9, 32, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv9_2')\n","\n","    conv10 = slim.conv2d(conv9, 12, [1, 1], rate=1, activation_fn=None, scope='g_conv10')\n","    out = tf.depth_to_space(conv10, 2)\n","    return out\n","\n","\n","def pack_raw(raw):\n","    # pack Bayer image to 4 channels\n","    im = raw.raw_image_visible.astype(np.float32)\n","    im = np.maximum(im - 512, 0) / (16383 - 512)  # subtract the black level\n","\n","    im = np.expand_dims(im, axis=2)\n","    img_shape = im.shape\n","    H = img_shape[0]\n","    W = img_shape[1]\n","\n","    out = np.concatenate((im[0:H:2, 0:W:2, :],\n","                          im[0:H:2, 1:W:2, :],\n","                          im[1:H:2, 1:W:2, :],\n","                          im[1:H:2, 0:W:2, :]), axis=2)\n","    return out\n","\n","# Added this for rerun\n","tf.reset_default_graph()\n","\n","sess = tf.Session()\n","in_image = tf.placeholder(tf.float32, [None, None, None, 4])\n","gt_image = tf.placeholder(tf.float32, [None, None, None, 3])\n","out_image = network(in_image)\n","\n","G_loss = tf.reduce_mean(tf.abs(out_image - gt_image))\n","\n","t_vars = tf.trainable_variables()\n","lr = tf.placeholder(tf.float32)\n","G_opt = tf.train.AdamOptimizer(learning_rate=lr).minimize(G_loss)\n","\n","saver = tf.train.Saver()\n","sess.run(tf.global_variables_initializer())\n","ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n","if ckpt:\n","    print('loaded ' + ckpt.model_checkpoint_path)\n","    saver.restore(sess, ckpt.model_checkpoint_path)\n","\n","# Raw data takes long time to load. Keep them in memory after loaded.\n","gt_images = [None] * 6000\n","input_images = {}\n","input_images['300'] = [None] * len(train_ids)\n","input_images['250'] = [None] * len(train_ids)\n","input_images['100'] = [None] * len(train_ids)\n","\n","g_loss = np.zeros((5000, 1))\n","\n","allfolders = glob.glob(result_dir + '*0')\n","lastepoch = 0\n","for folder in allfolders:\n","    lastepoch = np.maximum(lastepoch, int(folder[-4:]))\n","\n","learning_rate = 1e-4\n","for epoch in range(lastepoch, 4001):\n","    if os.path.isdir(result_dir + '%04d' % epoch):\n","        continue\n","    cnt = 0\n","    if epoch > 2000:\n","        learning_rate = 1e-5\n","\n","    for ind in np.random.permutation(len(train_ids)):\n","        # get the path from image id\n","        train_id = train_ids[ind]\n","        in_files = glob.glob(input_dir + '%05d_00*.ARW' % train_id)\n","        in_path = in_files[np.random.randint(0, len(in_files) - 1)]\n","        in_fn = os.path.basename(in_path)\n","\n","        gt_files = glob.glob(gt_dir + '%05d_00*.ARW' % train_id)\n","        gt_path = gt_files[0]\n","        gt_fn = os.path.basename(gt_path)\n","\n","        in_exposure = float(in_fn[9:-5])\n","        gt_exposure = float(gt_fn[9:-5])\n","\n","        ratio = min(gt_exposure / in_exposure, 300)\n","\n","        st = time.time()\n","        cnt += 1\n","\n","        if input_images[str(ratio)[0:3]][ind] is None:\n","            raw = rawpy.imread(in_path)\n","            input_images[str(ratio)[0:3]][ind] = np.expand_dims(pack_raw(raw), axis=0) * ratio\n","            gt_raw = rawpy.imread(gt_path)\n","            im = gt_raw.postprocess(use_camera_wb=True, half_size=False, no_auto_bright=True, output_bps=16)\n","            gt_images[ind] = np.expand_dims(np.float32(im / 65535.0), axis=0)\n","\n","        # crop\n","        H = input_images[str(ratio)[0:3]][ind].shape[1]\n","        W = input_images[str(ratio)[0:3]][ind].shape[2]\n","\n","        xx = np.random.randint(0, W - ps)\n","        yy = np.random.randint(0, H - ps)\n","        input_patch = input_images[str(ratio)[0:3]][ind][:, yy:yy + ps, xx:xx + ps, :]\n","        gt_patch = gt_images[ind][:, yy * 2:yy * 2 + ps * 2, xx * 2:xx * 2 + ps * 2, :]\n","\n","        if np.random.randint(2, size=1)[0] == 1:  # random flip\n","            input_patch = np.flip(input_patch, axis=1)\n","            gt_patch = np.flip(gt_patch, axis=1)\n","        if np.random.randint(2, size=1)[0] == 1:\n","            input_patch = np.flip(input_patch, axis=2)\n","            gt_patch = np.flip(gt_patch, axis=2)\n","        if np.random.randint(2, size=1)[0] == 1:  # random transpose\n","            input_patch = np.transpose(input_patch, (0, 2, 1, 3))\n","            gt_patch = np.transpose(gt_patch, (0, 2, 1, 3))\n","\n","        input_patch = np.minimum(input_patch, 1.0)\n","\n","        _, G_current, output = sess.run([G_opt, G_loss, out_image],\n","                                        feed_dict={in_image: input_patch, gt_image: gt_patch, lr: learning_rate})\n","        output = np.minimum(np.maximum(output, 0), 1)\n","        g_loss[ind] = G_current\n","        \n","        current_time = time.strftime(\"%H:%M:%S\", time.gmtime(time.time()))\n","        print(f\"File started at {start_time}, now at {current_time}.\")\n","        print(\"%d %d Loss=%.3f Time=%.3f\" % (epoch, cnt, np.mean(g_loss[np.where(g_loss)]), time.time() - st))\n","\n","        if epoch % save_freq == 0:\n","            if not os.path.isdir(result_dir + '%04d' % epoch):\n","                os.makedirs(result_dir + '%04d' % epoch)\n","\n","            temp = np.concatenate((gt_patch[0, :, :, :], output[0, :, :, :]), axis=1)\n","            toimage(temp * 255, high=255, low=0, cmin=0, cmax=255).save(\n","                result_dir + '%04d/%05d_00_train_%d.jpg' % (epoch, train_id, ratio))\n","\n","    saver.save(sess, checkpoint_dir + 'model.ckpt')"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}