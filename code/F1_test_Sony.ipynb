{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pvMy3FERt1de","executionInfo":{"status":"ok","timestamp":1682011970436,"user_tz":-120,"elapsed":9545,"user":{"displayName":"Floor Joosen","userId":"16910026288533856592"}},"outputId":"6ba55b40-188a-4a50-9833-e4d63a62dd16"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting rawpy\n","  Downloading rawpy-0.18.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from rawpy) (1.22.4)\n","Installing collected packages: rawpy\n","Successfully installed rawpy-0.18.0\n"]}],"source":["!pip install rawpy"]},{"cell_type":"code","source":["# uniform content loss + adaptive threshold + per_class_input + recursive G\n","# improvement upon cqf37\n","from __future__ import division\n","import os, scipy.io\n","#import tensorflow as tf\n","#import tensorflow.contrib.slim as slim\n","import numpy as np\n","import rawpy\n","import glob\n","\n","#Hi i added this\n","import tf_slim as slim\n","import tensorflow.compat.v1 as tf\n","tf.disable_v2_behavior()\n","from PIL import Image\n","\n","# Google colab finding files:\n","from google.colab import drive\n","drive.mount(\"/content/drive\", force_remount=True)\n","\n","# Note that you need a link to the shared folder in your google drive!\n","project_dir = '/content/drive/MyDrive/DLP/'\n","\n","picture_dir = '/content/drive/MyDrive/DLP/'\n","\n","link_dir = picture_dir + 'F1 pictures/'\n","run_name = 'F1_'\n","result_name = run_name\n","\n","input_dir = link_dir + 'short/'\n","gt_dir = link_dir + 'long/'\n","checkpoint_dir = project_dir + run_name + 'models/'\n","result_dir = project_dir + result_name + 'results/'\n","\n","print(checkpoint_dir)\n","print(result_dir)\n","\n","bit_depth = 10\n","\n","# get test IDs\n","test_fns = glob.glob(gt_dir + '/1*.dng')\n","test_ids = [int(os.path.basename(test_fn)[0:5]) for test_fn in test_fns]\n","\n","DEBUG = 0\n","if DEBUG == 1:\n","    save_freq = 2\n","    test_ids = test_ids[0:5]\n","\n","_errstr = \"Mode is unknown or incompatible with input array shape.\"\n","\n","# https://stackoverflow.com/questions/57545125/attributeerror-module-scipy-misc-has-no-attribute-toimage\n","def bytescale(data, cmin=None, cmax=None, high=255, low=0):\n","    \"\"\"\n","    Byte scales an array (image).\n","    Byte scaling means converting the input image to uint8 dtype and scaling\n","    the range to ``(low, high)`` (default 0-255).\n","    If the input image already has dtype uint8, no scaling is done.\n","    This function is only available if Python Imaging Library (PIL) is installed.\n","    Parameters\n","    ----------\n","    data : ndarray\n","        PIL image data array.\n","    cmin : scalar, optional\n","        Bias scaling of small values. Default is ``data.min()``.\n","    cmax : scalar, optional\n","        Bias scaling of large values. Default is ``data.max()``.\n","    high : scalar, optional\n","        Scale max value to `high`.  Default is 255.\n","    low : scalar, optional\n","        Scale min value to `low`.  Default is 0.\n","    Returns\n","    -------\n","    img_array : uint8 ndarray\n","        The byte-scaled array.\n","    Examples\n","    --------\n","    >>> from scipy.misc import bytescale\n","    >>> img = np.array([[ 91.06794177,   3.39058326,  84.4221549 ],\n","    ...                 [ 73.88003259,  80.91433048,   4.88878881],\n","    ...                 [ 51.53875334,  34.45808177,  27.5873488 ]])\n","    >>> bytescale(img)\n","    array([[255,   0, 236],\n","           [205, 225,   4],\n","           [140,  90,  70]], dtype=uint8)\n","    >>> bytescale(img, high=200, low=100)\n","    array([[200, 100, 192],\n","           [180, 188, 102],\n","           [155, 135, 128]], dtype=uint8)\n","    >>> bytescale(img, cmin=0, cmax=255)\n","    array([[91,  3, 84],\n","           [74, 81,  5],\n","           [52, 34, 28]], dtype=uint8)\n","    \"\"\"\n","    if data.dtype == np.uint8:\n","        return data\n","\n","    if high > 255:\n","        raise ValueError(\"`high` should be less than or equal to 255.\")\n","    if low < 0:\n","        raise ValueError(\"`low` should be greater than or equal to 0.\")\n","    if high < low:\n","        raise ValueError(\"`high` should be greater than or equal to `low`.\")\n","\n","    if cmin is None:\n","        cmin = data.min()\n","    if cmax is None:\n","        cmax = data.max()\n","\n","    cscale = cmax - cmin\n","    if cscale < 0:\n","        raise ValueError(\"`cmax` should be larger than `cmin`.\")\n","    elif cscale == 0:\n","        cscale = 1\n","\n","    scale = float(high - low) / cscale\n","    bytedata = (data - cmin) * scale + low\n","    return (bytedata.clip(low, high) + 0.5).astype(np.uint8)\n","\n","\n","def toimage(arr, high=255, low=0, cmin=None, cmax=None, pal=None,\n","            mode=None, channel_axis=None):\n","    \"\"\"Takes a numpy array and returns a PIL image.\n","    This function is only available if Python Imaging Library (PIL) is installed.\n","    The mode of the PIL image depends on the array shape and the `pal` and\n","    `mode` keywords.\n","    For 2-D arrays, if `pal` is a valid (N,3) byte-array giving the RGB values\n","    (from 0 to 255) then ``mode='P'``, otherwise ``mode='L'``, unless mode\n","    is given as 'F' or 'I' in which case a float and/or integer array is made.\n","    .. warning::\n","        This function uses `bytescale` under the hood to rescale images to use\n","        the full (0, 255) range if ``mode`` is one of ``None, 'L', 'P', 'l'``.\n","        It will also cast data for 2-D images to ``uint32`` for ``mode=None``\n","        (which is the default).\n","    Notes\n","    -----\n","    For 3-D arrays, the `channel_axis` argument tells which dimension of the\n","    array holds the channel data.\n","    For 3-D arrays if one of the dimensions is 3, the mode is 'RGB'\n","    by default or 'YCbCr' if selected.\n","    The numpy array must be either 2 dimensional or 3 dimensional.\n","    \"\"\"\n","    data = np.asarray(arr)\n","    if np.iscomplexobj(data):\n","        raise ValueError(\"Cannot convert a complex-valued array.\")\n","    shape = list(data.shape)\n","    valid = len(shape) == 2 or ((len(shape) == 3) and\n","                                ((3 in shape) or (4 in shape)))\n","    if not valid:\n","        raise ValueError(\"'arr' does not have a suitable array shape for \"\n","                         \"any mode.\")\n","    if len(shape) == 2:\n","        shape = (shape[1], shape[0])  # columns show up first\n","        if mode == 'F':\n","            data32 = data.astype(np.float32)\n","            image = Image.frombytes(mode, shape, data32.tostring())\n","            return image\n","        if mode in [None, 'L', 'P']:\n","            bytedata = bytescale(data, high=high, low=low,\n","                                 cmin=cmin, cmax=cmax)\n","            image = Image.frombytes('L', shape, bytedata.tostring())\n","            if pal is not None:\n","                image.putpalette(np.asarray(pal, dtype=np.uint8).tostring())\n","                # Becomes a mode='P' automagically.\n","            elif mode == 'P':  # default gray-scale\n","                pal = (np.arange(0, 256, 1, dtype=np.uint8)[:, np.newaxis] *\n","                       np.ones((3,), dtype=np.uint8)[np.newaxis, :])\n","                image.putpalette(np.asarray(pal, dtype=np.uint8).tostring())\n","            return image\n","        if mode == '1':  # high input gives threshold for 1\n","            bytedata = (data > high)\n","            image = Image.frombytes('1', shape, bytedata.tostring())\n","            return image\n","        if cmin is None:\n","            cmin = np.amin(np.ravel(data))\n","        if cmax is None:\n","            cmax = np.amax(np.ravel(data))\n","        data = (data*1.0 - cmin)*(high - low)/(cmax - cmin) + low\n","        if mode == 'I':\n","            data32 = data.astype(np.uint32)\n","            image = Image.frombytes(mode, shape, data32.tostring())\n","        else:\n","            raise ValueError(_errstr)\n","        return image\n","\n","    # if here then 3-d array with a 3 or a 4 in the shape length.\n","    # Check for 3 in datacube shape --- 'RGB' or 'YCbCr'\n","    if channel_axis is None:\n","        if (3 in shape):\n","            ca = np.flatnonzero(np.asarray(shape) == 3)[0]\n","        else:\n","            ca = np.flatnonzero(np.asarray(shape) == 4)\n","            if len(ca):\n","                ca = ca[0]\n","            else:\n","                raise ValueError(\"Could not find channel dimension.\")\n","    else:\n","        ca = channel_axis\n","\n","    numch = shape[ca]\n","    if numch not in [3, 4]:\n","        raise ValueError(\"Channel axis dimension is not valid.\")\n","\n","    bytedata = bytescale(data, high=high, low=low, cmin=cmin, cmax=cmax)\n","    if ca == 2:\n","        strdata = bytedata\n","        shape = (shape[1], shape[0])\n","    elif ca == 1:\n","        strdata = np.transpose(bytedata, (0, 2, 1))\n","        shape = (shape[2], shape[0])\n","    elif ca == 0:\n","        strdata = np.transpose(bytedata, (1, 2, 0))\n","        shape = (shape[2], shape[1])\n","    if mode is None:\n","        if numch == 3:\n","            mode = 'RGB'\n","        else:\n","            mode = 'RGBA'\n","\n","    if mode not in ['RGB', 'RGBA', 'YCbCr', 'CMYK']:\n","        raise ValueError(_errstr)\n","\n","    if mode in ['RGB', 'YCbCr']:\n","        if numch != 3:\n","            raise ValueError(\"Invalid array shape for mode.\")\n","    if mode in ['RGBA', 'CMYK']:\n","        if numch != 4:\n","            raise ValueError(\"Invalid array shape for mode.\")\n","\n","    # Here we know data and mode is correct\n","    image = Image.frombytes(mode, shape, strdata)\n","    return image\n","\n","\n","def lrelu(x):\n","    return tf.maximum(x * 0.2, x)\n","\n","\n","def upsample_and_concat(x1, x2, output_channels, in_channels):\n","    pool_size = 2\n","    deconv_filter = tf.Variable(tf.truncated_normal([pool_size, pool_size, output_channels, in_channels], stddev=0.02))\n","    deconv = tf.nn.conv2d_transpose(x1, deconv_filter, tf.shape(x2), strides=[1, pool_size, pool_size, 1])\n","\n","    deconv_output = tf.concat([deconv, x2], 3)\n","    deconv_output.set_shape([None, None, None, output_channels * 2])\n","\n","    return deconv_output\n","\n","\n","def network(input):\n","    conv1 = slim.conv2d(input, 32, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv1_1')\n","    conv1 = slim.conv2d(conv1, 32, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv1_2')\n","    pool1 = slim.max_pool2d(conv1, [2, 2], padding='SAME')\n","\n","    conv2 = slim.conv2d(pool1, 64, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv2_1')\n","    conv2 = slim.conv2d(conv2, 64, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv2_2')\n","    pool2 = slim.max_pool2d(conv2, [2, 2], padding='SAME')\n","\n","    conv3 = slim.conv2d(pool2, 128, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv3_1')\n","    conv3 = slim.conv2d(conv3, 128, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv3_2')\n","    pool3 = slim.max_pool2d(conv3, [2, 2], padding='SAME')\n","\n","    conv4 = slim.conv2d(pool3, 256, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv4_1')\n","    conv4 = slim.conv2d(conv4, 256, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv4_2')\n","    pool4 = slim.max_pool2d(conv4, [2, 2], padding='SAME')\n","\n","    conv5 = slim.conv2d(pool4, 512, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv5_1')\n","    conv5 = slim.conv2d(conv5, 512, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv5_2')\n","\n","    up6 = upsample_and_concat(conv5, conv4, 256, 512)\n","    conv6 = slim.conv2d(up6, 256, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv6_1')\n","    conv6 = slim.conv2d(conv6, 256, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv6_2')\n","\n","    up7 = upsample_and_concat(conv6, conv3, 128, 256)\n","    conv7 = slim.conv2d(up7, 128, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv7_1')\n","    conv7 = slim.conv2d(conv7, 128, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv7_2')\n","\n","    up8 = upsample_and_concat(conv7, conv2, 64, 128)\n","    conv8 = slim.conv2d(up8, 64, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv8_1')\n","    conv8 = slim.conv2d(conv8, 64, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv8_2')\n","\n","    up9 = upsample_and_concat(conv8, conv1, 32, 64)\n","    conv9 = slim.conv2d(up9, 32, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv9_1')\n","    conv9 = slim.conv2d(conv9, 32, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv9_2')\n","\n","    conv10 = slim.conv2d(conv9, 12, [1, 1], rate=1, activation_fn=None, scope='g_conv10')\n","    out = tf.depth_to_space(conv10, 2)\n","    return out\n","\n","\n","def pack_raw(raw):\n","    # pack Bayer image to 4 channels\n","    im = raw.raw_image_visible.astype(np.float32)\n","\n","    im = np.expand_dims(im, axis=2)\n","    im = im[:2800, :4000, :]\n","    img_shape = im.shape\n","    H = 2800\n","    W = 4000\n","\n","    out = np.concatenate((im[0:H:2, 0:W:2, :],\n","                          im[0:H:2, 1:W:2, :],\n","                          im[1:H:2, 1:W:2, :],\n","                          im[1:H:2, 0:W:2, :]), axis=2)\n","    \n","    black_levels = raw.black_level_per_channel\n","\n","    for i, bl in enumerate(black_levels):\n","      out[:,:,i] = np.maximum(out[:,:,i] - bl, 0) / (float(pow(2, bit_depth) - 1) - bl)  # subtract the black level\n","    return out\n","\n","# Added for reset before rerun\n","tf.reset_default_graph()\n","\n","sess = tf.Session()\n","in_image = tf.placeholder(tf.float32, [None, None, None, 4])\n","gt_image = tf.placeholder(tf.float32, [None, None, None, 3])\n","out_image = network(in_image)\n","\n","saver = tf.train.Saver()\n","sess.run(tf.global_variables_initializer())\n","ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n","if ckpt:\n","    print('loaded ' + ckpt.model_checkpoint_path)\n","    saver.restore(sess, ckpt.model_checkpoint_path)\n","\n","if not os.path.isdir(result_dir + 'final/'):\n","    os.makedirs(result_dir + 'final/')\n","\n","print(test_ids)\n","\n","for test_id in test_ids:\n","    # test the first image in each sequence\n","    in_files = glob.glob(input_dir + '%05d_00*.dng' % test_id)\n","    \n","    print(in_files)\n","\n","    for k in range(len(in_files)):\n","        in_path = in_files[k]\n","        in_fn = os.path.basename(in_path)\n","        print(in_fn)\n","        gt_files = glob.glob(gt_dir + '%05d_00*.dng' % test_id)\n","        gt_path = gt_files[0]\n","        gt_fn = os.path.basename(gt_path)\n","\n","        in_exposure = float(in_fn.split('_')[-1].split('s')[0])\n","        gt_exposure = float(gt_fn.split('_')[-1].split('s')[0])\n","        ratio = min(gt_exposure / in_exposure, 300)\n","\n","        raw = rawpy.imread(in_path)\n","        input_full = np.expand_dims(pack_raw(raw), axis=0) * ratio\n","\n","        im = raw.postprocess(use_camera_wb=True, half_size=False, no_auto_bright=True, output_bps=16)\n","        # scale_full = np.expand_dims(np.float32(im/65535.0),axis = 0)*ratio\n","        scale_full = np.expand_dims(np.float32(im / 65535.0), axis=0)\n","\n","        gt_raw = rawpy.imread(gt_path)\n","        im = gt_raw.postprocess(use_camera_wb=True, half_size=False, no_auto_bright=True, output_bps=16)\n","        gt_full = np.expand_dims(np.float32(im / 65535.0), axis=0)\n","\n","        input_full = np.minimum(input_full, 1.0)\n","\n","        output = sess.run(out_image, feed_dict={in_image: input_full})\n","        output = np.minimum(np.maximum(output, 0), 1)\n","\n","        output = output[0, :, :, :]\n","        gt_full = gt_full[0, :, :, :]\n","        scale_full = scale_full[0, :, :, :]\n","        scale_full = scale_full * np.mean(gt_full) / np.mean(\n","            scale_full)  # scale the low-light image to the same mean of the groundtruth\n","\n","        toimage(output * 255, high=255, low=0, cmin=0, cmax=255).save(result_dir + 'final/%5d_00_%d_out.png' % (test_id, ratio))\n","        toimage(scale_full * 255, high=255, low=0, cmin=0, cmax=255).save(result_dir + 'final/%5d_00_%d_scale.png' % (test_id, ratio))\n","        toimage(gt_full * 255, high=255, low=0, cmin=0, cmax=255).save(result_dir + 'final/%5d_00_%d_gt.png' % (test_id, ratio))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yGgyBFrXt-uW","executionInfo":{"status":"ok","timestamp":1682013141671,"user_tz":-120,"elapsed":1152856,"user":{"displayName":"Floor Joosen","userId":"16910026288533856592"}},"outputId":"e993dfe9-b4fb-43c8-ec1d-aac20ab41a45"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:From /usr/local/lib/python3.9/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","non-resource variables are not supported in the long term\n"]},{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/DLP/F1_models/\n","/content/drive/MyDrive/DLP/F1_results/\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n","  warnings.warn('`layer.apply` is deprecated and '\n"]},{"output_type":"stream","name":"stdout","text":["loaded /content/drive/MyDrive/DLP/F1_models/model.ckpt\n","[10017, 10018, 10021, 10022, 10023, 10001, 10002, 10010, 10007, 10005]\n","['/content/drive/MyDrive/DLP/F1 pictures/short/10017_00_0.017s.dng', '/content/drive/MyDrive/DLP/F1 pictures/short/10017_00_0.004s.dng']\n","10017_00_0.017s.dng\n","10017_00_0.004s.dng\n","['/content/drive/MyDrive/DLP/F1 pictures/short/10018_00_0.017s.dng', '/content/drive/MyDrive/DLP/F1 pictures/short/10018_00_0.004s.dng']\n","10018_00_0.017s.dng\n","10018_00_0.004s.dng\n","['/content/drive/MyDrive/DLP/F1 pictures/short/10021_00_0.017s.dng', '/content/drive/MyDrive/DLP/F1 pictures/short/10021_00_0.004s.dng']\n","10021_00_0.017s.dng\n","10021_00_0.004s.dng\n","['/content/drive/MyDrive/DLP/F1 pictures/short/10022_00_0.017s.dng', '/content/drive/MyDrive/DLP/F1 pictures/short/10022_00_0.004s.dng']\n","10022_00_0.017s.dng\n","10022_00_0.004s.dng\n","['/content/drive/MyDrive/DLP/F1 pictures/short/10023_00_0.017s.dng', '/content/drive/MyDrive/DLP/F1 pictures/short/10023_00_0.004s.dng']\n","10023_00_0.017s.dng\n","10023_00_0.004s.dng\n","['/content/drive/MyDrive/DLP/F1 pictures/short/10001_00_0.056s.dng', '/content/drive/MyDrive/DLP/F1 pictures/short/10001_00_0.043s.dng']\n","10001_00_0.056s.dng\n","10001_00_0.043s.dng\n","['/content/drive/MyDrive/DLP/F1 pictures/short/10002_00_0.056s.dng', '/content/drive/MyDrive/DLP/F1 pictures/short/10002_00_0.043s.dng']\n","10002_00_0.056s.dng\n","10002_00_0.043s.dng\n","['/content/drive/MyDrive/DLP/F1 pictures/short/10010_00_0.056s.dng', '/content/drive/MyDrive/DLP/F1 pictures/short/10010_00_0.043s.dng']\n","10010_00_0.056s.dng\n","10010_00_0.043s.dng\n","['/content/drive/MyDrive/DLP/F1 pictures/short/10007_00_0.043s.dng', '/content/drive/MyDrive/DLP/F1 pictures/short/10007_00_0.125s.dng']\n","10007_00_0.043s.dng\n","10007_00_0.125s.dng\n","['/content/drive/MyDrive/DLP/F1 pictures/short/10005_00_0.033s.dng', '/content/drive/MyDrive/DLP/F1 pictures/short/10005_00_0.017s.dng']\n","10005_00_0.033s.dng\n","10005_00_0.017s.dng\n"]}]}]}